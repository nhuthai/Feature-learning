# Feature learning
## 1. Self-attention (A. Vaswani, 2017)
link for explanation: https://www.linkedin.com/pulse/explanation-self-attention-single-head-nhut-hai-huynh/
![alt text](https://github.com/nhuthai/Feature-learning/blob/master/SAN/imgs/architecture.PNG)
![alt text](https://github.com/nhuthai/Feature-learning/blob/master/SAN/imgs/head_loop10.PNG)
![alt text](https://github.com/nhuthai/Feature-learning/blob/master/SAN/imgs/head_loop50.PNG)
![alt text](https://github.com/nhuthai/Feature-learning/blob/master/SAN/imgs/head_loop100.PNG)
![alt text](https://github.com/nhuthai/Feature-learning/blob/master/SAN/imgs/multihead.PNG)
## 2. Deep Infomax (D. Hjelm, 2019)

# References
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, **Attention is all you need.** in *NIPS 2017*, 2017.\
D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bachman, A. Trischler, Y. Bengio, **Learning deep representations by mutual information estimation and maximization** in *ICLR 2019*, April 2019.
